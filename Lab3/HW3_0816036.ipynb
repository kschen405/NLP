{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Terry\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertForMultipleChoice\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import datasets\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import AlbertForMultipleChoice, AutoModelForMultipleChoice\n",
    "import transformers\n",
    "# transformers.logging.set_verbosity_error()\n",
    "from transformers import logging\n",
    "import csv\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'漢字'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opencc\n",
    "cc = opencc.OpenCC('s2t.json')\n",
    "cc.convert('汉字')  # 漢字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim1: n, index of sample\n",
    "# dim2: 0, context ; 1, {question choice answer}*m ; 2, X\n",
    "# dim3: 0, X       ; 1, {question choice answer}   ; 2, X\n",
    "file = open('train_HW3dataset.json',encoding=\"utf-8\")\n",
    "train = json.load(file)\n",
    "file.close()\n",
    "file = open('dev_HW3dataset.json',encoding=\"utf-8\")\n",
    "dev = json.load(file)\n",
    "file.close()\n",
    "file = open('test_HW3dataset.json',encoding=\"utf-8\")\n",
    "test = json.load(file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小女孩怕麻烦', '小女孩认为孕妇怕麻烦', '孕妇怕麻烦', '孕妇很聪明']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][1][0]['choice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_index = [ [choice2_sample_index_lst] [choice3_sample_index_lst] [choice4_sample_index_lst] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = [[], [], []]\n",
    "\n",
    "def output_json(samples, outfile_name=\"train.json\", test_file=False):\n",
    "    data = []\n",
    "    test_data = [[], [], []]\n",
    "\n",
    "    index = 0\n",
    "    for sample in samples:\n",
    "        for qac in sample[1]:\n",
    "            choice_lst = [ cc.convert(choice) for choice in qac['choice'] ]\n",
    "            if not test_file:\n",
    "                train_dict = {'context': cc.convert(sample[0][0]), 'question': cc.convert(qac['question']), 'label': qac['choice'].index(qac['answer']), 'choices': choice_lst}\n",
    "                data.append(train_dict)\n",
    "            else:\n",
    "                train_dict = {'context': cc.convert(sample[0][0]), 'question': cc.convert(qac['question']), 'label': \"\",'choices': choice_lst}\n",
    "                test_data[len(qac['choice']) - 2].append(train_dict)\n",
    "                test_index[len(qac['choice']) - 2].append(index)\n",
    "                index += 1\n",
    "\n",
    "    if not test_file:\n",
    "        output_dict = {\"data\": data}\n",
    "        with open(outfile_name, \"w\") as outfile:\n",
    "            json.dump(output_dict, outfile, indent=2)#, ensure_ascii=False)\n",
    "    else:\n",
    "        for num_choice in range(2,5):\n",
    "            # data_choice = {k: v for k, v in data.items() if len(v['choices']) == num_choice }\n",
    "\n",
    "            output_dict = {\"data\": test_data[num_choice-2]}\n",
    "            with open(f\"{outfile_name[:-5]}{num_choice}.json\" , \"w\") as outfile:\n",
    "                json.dump(output_dict, outfile, indent=2) #, ensure_ascii=False)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "output_json(train, outfile_name=\"train.json\")\n",
    "output_json(dev, outfile_name=\"dev.json\")\n",
    "output_json(test, outfile_name=\"test.json\", test_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '在這個重視外表的時代，尤其是在文藝界裏，周杰倫的成功有點兒不可思議。就像他自己所說：“我大部分的女歌迷都不會對我說我很帥，相反地，她們會告訴我，她們喜歡我的音樂，被我的音樂吸引。”\\n據母親回憶，周杰倫在學會走路前，就對音樂很敏感。周媽媽在他四歲的時候，就送他進鋼琴班學琴，而且他十分努力，對鋼琴很瘋狂。高中鋼琴老師說，周杰倫十幾歲時，就可以當衆表演。出了練琴的房問，周杰倫是個再普通不過的青少年，打籃球、看功夫電影、打遊戲，在學習上並不優秀，大學都很難考上。\\n是音樂救了他，給他帶來了幸運。有一次，朋友幫他報名參加一個節目，他不敢一個人表演，決定幫一位想當歌手的朋友寫歌，併爲他彈鋼琴。那位想當歌手的朋友唱得很糟糕，但是主持人看了周杰倫寫的歌，立即找到他，與他簽了合同，讓他專門爲歌手寫歌。\\n就這樣，他用兩年時間專心爲歌手寫歌，他寫的歌總能流行起來。後來，他又從幕後走到臺前，成爲亞洲最流行的歌手之一。',\n",
       " 'question': '周杰倫對什麼很敏感?',\n",
       " 'label': '',\n",
       " 'choices': ['籃球', '功夫', '音樂', '遊戲']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('test4.json',encoding=\"utf-8\")\n",
    "ttrain = json.load(file)\n",
    "file.close()\n",
    "ttrain['data'][100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fb3ad7adaa55915d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Terry/.cache/huggingface/datasets/json/default-fb3ad7adaa55915d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 2004.93it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Terry/.cache/huggingface/datasets/json/default-fb3ad7adaa55915d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 167.23it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 130.06ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 118.04ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 149.44ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 125.42ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 145.84ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 118.04ba/s]\n",
      "Using custom data configuration default-c153f55b5e3353c7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Terry/.cache/huggingface/datasets/json/default-c153f55b5e3353c7/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1005.11it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Terry/.cache/huggingface/datasets/json/default-c153f55b5e3353c7/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 167.22it/s]\n",
      "Using custom data configuration default-b758766aec7fceb6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Terry/.cache/huggingface/datasets/json/default-b758766aec7fceb6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Terry/.cache/huggingface/datasets/json/default-b758766aec7fceb6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 167.20it/s]\n",
      "Using custom data configuration default-8b04f5dda9f24bb1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Terry/.cache/huggingface/datasets/json/default-8b04f5dda9f24bb1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1004.14it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Terry/.cache/huggingface/datasets/json/default-8b04f5dda9f24bb1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 167.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def split_by_choices(data_ds, num_choice):\n",
    "    split_data_ds = data_ds.filter(lambda example:  len(example['choices']) == num_choice )\n",
    "    return split_data_ds\n",
    "    \n",
    "data_ds = load_dataset(\"json\", data_files={\"train\":\"train.json\", \"dev\": \"dev.json\"}, field=\"data\")\n",
    "data_ds2 = split_by_choices(data_ds, 2)\n",
    "data_ds3 = split_by_choices(data_ds, 3)\n",
    "data_ds4 = split_by_choices(data_ds, 4)\n",
    "\n",
    "test_data_ds2 = load_dataset(\"json\", data_files={\"test\":\"test2.json\"}, field=\"data\")\n",
    "test_data_ds3 = load_dataset(\"json\", data_files={\"test\":\"test3.json\"}, field=\"data\")\n",
    "test_data_ds4 = load_dataset(\"json\", data_files={\"test\":\"test4.json\"}, field=\"data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'label', 'choices'],\n",
       "        num_rows: 1672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data_ds4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"ckiplab/bert-base-chinese\")\n",
    "\n",
    "\n",
    "# print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.39ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:04,  1.03ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:03,  1.08ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.07ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.06ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.02s/ba]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_choice =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.17ba/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    num_choice = len(examples[\"choices\"][0])\n",
    "    print(\"num_choice = \", num_choice)\n",
    "\n",
    "    contexts_questions = [[f\"{context}[SEP]{question}\"]*num_choice for context, question in zip(examples[\"context\"], examples[\"question\"])]\n",
    "    choices = examples[\"choices\"]\n",
    "    contexts_questions = sum(contexts_questions, [])\n",
    "    choices = sum(choices, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(contexts_questions, choices, truncation=True)\n",
    "    \n",
    "    # print(tokenized_examples.keys())\n",
    "    \n",
    "    return {k: [v[i : i + num_choice] for i in range(0, len(v), num_choice)] for k, v in tokenized_examples.items() }\n",
    "\n",
    "# train\n",
    "tokenized_data_ds2 = data_ds2.map(preprocess_function, batched=True)\n",
    "tokenized_data_ds3 = data_ds3.map(preprocess_function, batched=True)\n",
    "tokenized_data_ds4 = data_ds4.map(preprocess_function, batched=True)\n",
    "# # test\n",
    "# tokenized_test_data_ds2 = test_data_ds2.map(preprocess_function, batched=True)\n",
    "# tokenized_test_data_ds3 = test_data_ds3.map(preprocess_function, batched=True)\n",
    "# tokenized_test_data_ds4 = test_data_ds4.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 許 多 動 物 的 某 些 器 官 感 覺 特 別 靈 敏 ， 它 們 能 比 人 類 提 前 知 道 一 些 災 害 事 件 的 發 生 ， 例 如 ， 海 洋 中 的 水 母 能 預 報 風 暴 ， 老 鼠 能 事 先 躲 避 礦 井 崩 塌 或 有 害 氣 體 ， 等 等 。 地 震 往 往 能 使 一 些 動 物 的 某 些 感 覺 器 官 受 到 刺 激 而 發 生 異 常 反 應 。 如 一 個 地 區 的 重 力 發 生 變 異 ， 某 些 動 物 可 能 通 過 它 們 的 平 衡 器 官 感 覺 到 ； 一 種 振 動 異 常 ， 某 些 動 物 的 聽 覺 器 官 也 許 能 夠 察 覺 出 來 。 地 震 前 地 下 岩 層 早 已 在 逐 日 緩 慢 活 動 ， 而 斷 層 面 之 間 又 具 有 強 大 的 摩 擦 力 。 這 種 摩 擦 力 會 產 生 一 種 低 於 人 的 聽 覺 所 能 感 覺 到 的 低 頻 聲 波 。 人 對 每 秒 20 次 以 上 的 聲 波 才 能 感 覺 到 ， 而 動 物 則 不 然 。 那 些 感 覺 十 分 靈 敏 的 動 物 ， 在 感 觸 到 這 種 低 聲 波 時 ， 便 會 驚 恐 萬 狀 ， 以 至 出 現 冬 蛇 出 洞 、 魚 躍 水 面 等 異 常 現 象 。 [SEP] 動 物 的 器 官 感 覺 與 人 的 相 比 有 什 麼 不 同? [SEP] 沒 有 人 的 靈 敏 [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_data_ds4['train']['input_ids'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'label', 'choices', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 553\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['context', 'question', 'label', 'choices', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_ds = {2:tokenized_data_ds2, 3:tokenized_data_ds3, 4:tokenized_data_ds4}\n",
    "tokenized_data_ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    num_choices: int = 2\n",
    "\n",
    "    def __call__(self, features):\n",
    "        labels = [feature.pop(\"label\") for feature in features]\n",
    "        batch_size = len(features)\n",
    "        # num_choices = len(features[0][\"choices\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(self.num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, self.num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForMultipleChoice.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "\n",
    "model_dict = {}\n",
    "for num_choice in range(2,5):\n",
    "    model_dict[num_choice] = AutoModelForMultipleChoice.from_pretrained(\"ckiplab/albert-base-chinese\")\n",
    "    # model_dict[num_choice] = AutoModelForMultipleChoice.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `AlbertForMultipleChoice.forward` and have been ignored: context, choices, question. If context, choices, question are not expected by `AlbertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5045\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6310\n",
      "  Number of trainable parameters = 10548737\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3861, 'learning_rate': 9.207606973058639e-06, 'epoch': 0.4}\n",
      "{'loss': 1.2849, 'learning_rate': 8.415213946117275e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForMultipleChoice.forward` and have been ignored: context, choices, question. If context, choices, question are not expected by `AlbertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1669\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to saved_model_wwm4\\checkpoint-1262\n",
      "Configuration saved in saved_model_wwm4\\checkpoint-1262\\config.json\n",
      "Model weights saved in saved_model_wwm4\\checkpoint-1262\\pytorch_model.bin\n",
      "tokenizer config file saved in saved_model_wwm4\\checkpoint-1262\\tokenizer_config.json\n",
      "Special tokens file saved in saved_model_wwm4\\checkpoint-1262\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2080864906311035, 'eval_accuracy': 0.4907130017974835, 'eval_runtime': 91.6655, 'eval_samples_per_second': 18.208, 'eval_steps_per_second': 4.56, 'epoch': 1.0}\n",
      "{'loss': 1.1708, 'learning_rate': 7.622820919175912e-06, 'epoch': 1.19}\n",
      "{'loss': 1.0991, 'learning_rate': 6.830427892234549e-06, 'epoch': 1.58}\n",
      "{'loss': 1.067, 'learning_rate': 6.038034865293186e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForMultipleChoice.forward` and have been ignored: context, choices, question. If context, choices, question are not expected by `AlbertForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1669\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "training_args =  [\n",
    "    TrainingArguments(\n",
    "    output_dir=f\"saved_model_wwm{num_choice}\",#\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-5,#\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    "    )\n",
    "    for num_choice in range(2,5)\n",
    "]\n",
    "\n",
    "trainers = [\n",
    "    Trainer(\n",
    "        model=model_dict[num_choice],\n",
    "        args=training_args[num_choice-2],\n",
    "        train_dataset=tokenized_data_ds[num_choice]['train'],\n",
    "        eval_dataset=tokenized_data_ds[num_choice][\"dev\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer, num_choices = num_choice ),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    for num_choice in range(2,5) \n",
    "]\n",
    "\n",
    "# for i in range(len(trainers)):\n",
    "#     trainers[i].train()\n",
    "\n",
    "# trainers[0].train()\n",
    "# trainers[1].train()\n",
    "trainers[2].train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 昨 天 的 考 試 我 半 個 小 時 就 做 完 了 ， 但 是 很 多 不 應 該 錯 的 都 做 錯 了 ， 因 爲 考 試 的 時 候 覺 得 太 簡 單 了 ， 沒 有 認 真 檢 查 。 [SEP] 昨 天 的 考 試 他 考 得 不 錯 。 [SEP] 正 確 [SEP]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_data_ds2['train']['input_ids'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dict = {2: test_data_ds2['test'], 3:test_data_ds3['test'], 4:test_data_ds4['test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錯誤\n"
     ]
    }
   ],
   "source": [
    "for feature in test_dataset_dict[2]:\n",
    "    print(cc.convert(feature['choices'][1]))\n",
    "    # print(feature.items())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file saved_model_wwm2\\checkpoint-417_loss0.68_acc0.59\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"ckiplab/albert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMultipleChoice\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layers_to_keep\": [],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file saved_model_wwm2\\checkpoint-417_loss0.68_acc0.59\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForMultipleChoice.\n",
      "\n",
      "All the weights of AlbertForMultipleChoice were initialized from the model checkpoint at saved_model_wwm2\\checkpoint-417_loss0.68_acc0.59.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMultipleChoice for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: CHOICE 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file saved_model_wwm3\\checkpoint-208_loss1.05_acc0.46\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"ckiplab/albert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMultipleChoice\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layers_to_keep\": [],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file saved_model_wwm3\\checkpoint-208_loss1.05_acc0.46\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForMultipleChoice.\n",
      "\n",
      "All the weights of AlbertForMultipleChoice were initialized from the model checkpoint at saved_model_wwm3\\checkpoint-208_loss1.05_acc0.46.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMultipleChoice for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: CHOICE 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file saved_model_wwm4\\checkpoint-2524_loss1.14_acc0.57\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"ckiplab/albert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMultipleChoice\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layers_to_keep\": [],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file saved_model_wwm4\\checkpoint-2524_loss1.14_acc0.57\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForMultipleChoice.\n",
      "\n",
      "All the weights of AlbertForMultipleChoice were initialized from the model checkpoint at saved_model_wwm4\\checkpoint-2524_loss1.14_acc0.57.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMultipleChoice for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: CHOICE 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "def inference(test_dataset_dict: dict, model_path: list):\n",
    "    index_lst = []\n",
    "    pred_lst = []\n",
    "    for num_choice in range(2,5):\n",
    "        print(f\"START: CHOICE {num_choice}\")\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_path[num_choice-2]) #, device_map='auto', load_in_8bit=True)\n",
    "        model = AlbertForMultipleChoice.from_pretrained(model_path[num_choice-2]).to(\"cuda\") # , device_map='auto', load_in_8bit=True)\n",
    "        \n",
    "        for i, feature in enumerate(test_dataset_dict[num_choice]):\n",
    "            # for k, v in feature.items():\n",
    "                # feature[k] = feature[k].to(\"cuda\")\n",
    "                \n",
    "            prompt = f\"{feature['context']}[SEP]{feature['question']}\"\n",
    "            prompt_candidates = [[prompt, feature['choices'][i]] for i in range(num_choice)]\n",
    "            inputs = tokenizer(prompt_candidates, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            outputs = model(**{k: v.unsqueeze(0).to(\"cuda\") for k, v in inputs.items()})\n",
    "            pred = outputs.logits.argmax().item()\n",
    "            index_lst.append(test_index[num_choice-2][i])\n",
    "            pred_lst.append(pred+1) # \n",
    "        model = None\n",
    "    output_dict = {'index': index_lst, 'answer': pred_lst}\n",
    "    output_df = pd.DataFrame(output_dict) \n",
    "    output_df.to_csv(\"HW3_submission.csv\", index=False)\n",
    "\n",
    "model_path=['saved_model_wwm2\\checkpoint-417_loss0.68_acc0.59', 'saved_model_wwm3\\checkpoint-208_loss1.05_acc0.46', 'saved_model_wwm4\\checkpoint-2524_loss1.14_acc0.57']\n",
    "inference(test_dataset_dict, model_path)       \n",
    "# outputs = model_dict[2](tokenized_test_data_ds2['test2']['input_ids']) #, labels=labels)  # batch size is 1\n",
    "\n",
    "# # the linear classifier still needs to be trained\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "writer.writerow() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[0;32m      3\u001b[0m     writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(outfile)\n\u001b[1;32m----> 4\u001b[0m     writer\u001b[39m.\u001b[39;49mwriterow()\n\u001b[0;32m      5\u001b[0m     w\u001b[39m.\u001b[39mwriterow(test_dict)\n",
      "\u001b[1;31mTypeError\u001b[0m: writer.writerow() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "test_dict = {'index':[1,2,3,4,5], 'answer':[9,8,7,6,5]}\n",
    "with open(\"test.csv\", \"w\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow()\n",
    "    w.writerow(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29931e1b5fd008239d71c860573e1ff80df036f9c7b6fafc66d2e0549fca9a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
